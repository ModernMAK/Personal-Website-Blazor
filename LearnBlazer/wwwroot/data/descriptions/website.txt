This website was built to showcase my projects, but it was an interesting project in it's own right, requiring a lot of experience from past projects, namely LiteSpeed, FileTagServer, and some miscellaneous work with AWS.

While WordPress or Blogger work well for blogs, they can feel bloated with extra features you likely don't need in a small website. And depending on your hosting provider; can be extremely tedious to set up if you don't have full control over the machine. So while it was certainly possible to use these services; I opted for a simple approach which I could set up in a day.

I decided to build my server using FastAPI. It's designed to make RESTful APIs quickly in python, but it functions just as well as a web server. Adding pages is simple; just add a python decorator and return some HTML. With PyStache, I can create template pages to keep pages consistent, and avoid rebuilding HTML pages for every article I need. By utilizing Bootstrap, I can make my pages responsive without any extra JavaScript on my part. With Uvicorn running the FastAPI application, I'm ready to launch my server. 

You may have noticed I did not mention SQL; and that is the big difference between this and solutions like WordPress or Blogger. SQL is great, it's scalable and fast, but it also needs to be set up and configured properly. I'm not tracking users, comments by users, articles or article metadata; all information that would typically be tracked in a SQL database. Put simply, I just don't need the scalability SQL would grant me. Which is why I use JSON and python dictionaries; it's easily modifiable, I can still perform lookups as needed, and with only 20-some items, lookup performance is not an issue.

With the actual website done, all that's left is to get it on the internet. I decided to run this using Amazon Web Services, to allow me full access to the machine, and to avoid anything extra a hosting provider would require of me. I simply spun up an instance of AWS-linux and got to work, assigning a static ip, and updating my A-records with my domain provider. After an hour of DNS-propagation I was able to see that everything had worked, and I now had a server running on my website; a blank one, it was time to get it running.

My first solution was to simply git clone the repo from GitHub and build using SSH, but I felt this was too cumbersome; at the time I had decided to keep the website repo private, and getting PuttyGen to play nice by generating keys for GitHub was not something I wanted to spend a few hours on. After caving and making the repo public, I discovered I couldn't bind to port 80. While I now know this was because port 80 is privileged, and requires root to assign, at the time I was stumped.

This was about the time I decided I wanted to use Docker. Docker would allow me to ensure that the image I was working with on the server was the same image as my local copy. With DockerHub, I could quickly push/pull my images as needed. Docker also kept everything contained, allowing me to quickly scrap the server and restart it when I needed to do an update. But most importantly, because docker was a service running under root, it had permissions to bind to port 80. After a lot of fiddling with dockerfiles, I was able to create a rather simple workflow to spin up my site; pull from github, build the image, then run it; 3 easy steps.

With my site running HTTP, I felt pretty confident, but I decided to go the extra mile and implement HTTPS. I had a bit of familiarity with how HTTPS worked, but at the time I didn't know how to set it up. And while I had a lot of hardship at first, having done it once, it's quite simple. LetsEncrypt, the de-facto website for distributing SSL certificates, makes it extremely easy with Cert-bot; a small program which automates the process. After running their Cert-Bot docker image I needed to send the generated keys to Uvicorn, which would handle the HTTPS aspect of my app for me. This was the most harrowing part of this experience, because I did not realize that Cert-Bot uses symbolic links to reference the actual certificates in the file system. Because I was using docker, I needed to mount the drives when running my website. Because these files were symbolic links, and I was trying to minimize how much of the file system I let my server access, the sym-links were invalid in my container, linking to files which docker could not see. Realizing that, I was able to give docker access to the SSL certificate, and my website was now running on HTTPS.

But now I was no longer managing any HTTP requests. At first I used docker to run the image on port 80 as well, but decided against this since it was just hiding all the work I'd spent to set up HTTPS. This was about the time when I wished I wasn't using docker, so I could rely on NGinX instead. With NGinX, I could set up a configuration to redirect traffic, run it, and forget it. Unfortunately, I've never paired NGinX with Docker, and I wasn't going to spend hours of my time attempting to when I could simply set up a separate FastAPI server to redirect traffic. This worked well as a band-aid fix, but it led to a few wierd cases where redirects simply didn't work; which finally convinced me to just use nginx. Rather than run nginx on the machine and drop docker, I decided to run nginx in docker with my webserver. This way I avoid the headache of migrating away from docker, while still being able to use nginx. Migrating to NGinX was extremely simple too, since I was already using uvicorn all I had to do was setup my nginx configs and update my docker file. A painless transition.

Now with the help of a few bash scripts and a cron job, I may never need to touch the server again! At least until I decide to add more features.